{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Normalization\n",
    "#text = text.split()\n",
    "#lowercase the text \n",
    "test = ['this','is','just','A','test']\n",
    "def lower(list):\n",
    "    lower_text = []\n",
    "    for word in list:\n",
    "        word = word.lower()\n",
    "        lower_text.append(word)\n",
    "    return lower_text\n",
    "\n",
    "a = lower(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of punctuation \n",
    "def alphanum(list):\n",
    "    lower_alnum_list = []\n",
    "    for item in list:\n",
    "        if item.isalnum():\n",
    "            lower_alnum_list.append(item)\n",
    "    return lower_alnum_list \n",
    "\n",
    "b = alphanum(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'just', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "#tokenize the text \n",
    "l = \" \".join(b)\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "tokens = word_tokenize(l)\n",
    "print(tokens)\n",
    "#ttr function\n",
    "def ttr(tokens):\n",
    "    ttr = len(set(tokens))/len(tokens)\n",
    "    return ttr \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type token ratio in the research paper corpus is 0.11187155868882828\n",
      "38866\n"
     ]
    }
   ],
   "source": [
    "#calculating ttr in research paper corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "respaper = open('/Users/alexwang/Documents/NLP/Assignment1/respapercorpus.txt').read()\n",
    "respaper_splited = respaper.split(\" \")\n",
    "respaper_lower = lower(respaper_splited)\n",
    "respaper_lower_wtpun = \" \".join(alphanum(respaper_lower))\n",
    "\n",
    "tokens_res = word_tokenize(respaper_lower_wtpun)\n",
    "print(\"the type token ratio in the research paper corpus is\", ttr(tokens_res))\n",
    "print(len(tokens_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type token ratio in the argumentative essay corpus is 0.13279887482419128\n"
     ]
    }
   ],
   "source": [
    "#calculating ttr in argumentative essay corpus \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "argessay = open('/Users/alexwang/Documents/NLP/Assignment1/argessaycorpus.txt').read()\n",
    "argessay_splited = argessay.split(\" \")\n",
    "argessay_lower = lower(argessay_splited)\n",
    "argessay_lower_wtpun = \" \".join(alphanum(argessay_lower))\n",
    "\n",
    "tokens_arg = word_tokenize(argessay_lower_wtpun)\n",
    "print(\"the type token ratio in the argumentative essay corpus is\", ttr(tokens_arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2776.1428571428573\n"
     ]
    }
   ],
   "source": [
    "#calculating average # of words in research paper corpus \n",
    "# the # of research paper is 14 \n",
    "ave_words_res = len(tokens_res)/14\n",
    "print(ave_words_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2221.875\n"
     ]
    }
   ],
   "source": [
    "#calculating average # of words in argumentative essay corpus \n",
    "# the # of argumenatative essay is 16\n",
    "ave_words_arg = len(tokens_arg)/16\n",
    "print(ave_words_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average word length in research paper corpus is 5.980111151134667\n"
     ]
    }
   ],
   "source": [
    "#average length of words in the research paper corpus \n",
    "\n",
    "ave_len_words_res = len(list(respaper_lower_wtpun))/len(tokens_res)\n",
    "print(\"the average word length in research paper corpus is\" ,ave_len_words_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average word length in argumentative essay corpus is 5.993389592123769\n"
     ]
    }
   ],
   "source": [
    "#average length of words in the argumentative essay corpus \n",
    "\n",
    "ave_len_words_arg = len(list(argessay_lower_wtpun))/len(tokens_arg)\n",
    "print(\"the average word length in argumentative essay corpus is\" ,ave_len_words_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average length of sentences in the research paper corpus is 20.50976253298153\n"
     ]
    }
   ],
   "source": [
    "#average length of sentences in the research paper copurs\n",
    "from nltk.tokenize import sent_tokenize \n",
    "ave_sent_len_res = len(tokens_res)/len(sent_tokenize(respaper))\n",
    "print('the average length of sentences in the research paper corpus is' , ave_sent_len_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average length of sentences in the argumentative essay corpus is 19.99437570303712\n"
     ]
    }
   ],
   "source": [
    "#average length of sentences in the argumentative essay corpus \n",
    "from nltk.tokenize import sent_tokenize \n",
    "ave_sent_len_arg = len(tokens_arg)/len(sent_tokenize(argessay))\n",
    "print('the average length of sentences in the argumentative essay corpus is', ave_sent_len_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the content words ratio in research paper corpus is 0.5663819276488448\n",
      "the content words ratio in argumentative essay corpus is 0.5646413502109705\n"
     ]
    }
   ],
   "source": [
    "#pos ratio \n",
    "#content words over total tokens \n",
    "\n",
    "from nltk import pos_tag \n",
    "\n",
    "content_words_tag =['JJ','JJR','JJS','NN','NNS','NNP','NNPS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "#calculate research paper corpus content words ratio\n",
    "res_tag = pos_tag(tokens_res)\n",
    "\n",
    "content_res = []\n",
    "for item in res_tag:\n",
    "    if item[1] in content_words_tag:\n",
    "        content_res.append(item[0])\n",
    "        \n",
    "res_content_ratio = len(content_res)/len(tokens_res)\n",
    "print('the content words ratio in research paper corpus is' , res_content_ratio)\n",
    "\n",
    "#calculate argumentative essay corpus content words ratio\n",
    "arg_tag = pos_tag(tokens_arg)\n",
    "\n",
    "content_arg = []\n",
    "for item in arg_tag:\n",
    "    if item[1] in content_words_tag:\n",
    "        content_arg.append(item[0])\n",
    "\n",
    "arg_content_ratio = len(content_arg)/len(tokens_arg)\n",
    "print('the content words ratio in argumentative essay corpus is' ,arg_content_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292186\n",
      "267330\n"
     ]
    }
   ],
   "source": [
    "print(len(respaper))\n",
    "print(len(argessay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38866\n",
      "35550\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_res))\n",
    "print(len(tokens_arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594059405940594\n",
      "0.7029702970297029\n"
     ]
    }
   ],
   "source": [
    "# the first 100 tokens ttr in research paper corpus \n",
    "ttr_res_100 = len(set(tokens_res[:101]))/len(tokens_res[:101])\n",
    "print(ttr_res_100)\n",
    "\n",
    "# the first 100 tokens ttr in argumentative essay corpus\n",
    "ttr_arg_100 = len(set(tokens_arg[:101]))/len(tokens_arg[:101])\n",
    "print(ttr_arg_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
