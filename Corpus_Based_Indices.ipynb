{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Normalization\n",
    "#text = text.split()\n",
    "#lowercase the text \n",
    "test = ['this','is','just','A','test']\n",
    "def lower(list):\n",
    "    lower_text = []\n",
    "    for word in list:\n",
    "        word = word.lower()\n",
    "        lower_text.append(word)\n",
    "    return lower_text\n",
    "\n",
    "a = lower(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of punctuation \n",
    "def alphanum(list):\n",
    "    lower_alnum_list = []\n",
    "    for item in list:\n",
    "        if item.isalnum():\n",
    "            lower_alnum_list.append(item)\n",
    "    return lower_alnum_list \n",
    "\n",
    "b = alphanum(a)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'just', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "#tokenize the text \n",
    "l = \" \".join(b)\n",
    "from nltk.tokenize import word_tokenize \n",
    "tokens = word_tokenize(l)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "respaper = open('/Users/alexwang/Documents/NLP/Assignment1/respapercorpus.txt').read()\n",
    "respaper_splited = respaper.split(\" \")\n",
    "respaper_lower = lower(respaper_splited)\n",
    "respaper_lower_wtpun = \" \".join(alphanum(respaper_lower))\n",
    "\n",
    "tokens_res = word_tokenize(respaper_lower_wtpun)\n",
    "\n",
    "argessay = open('/Users/alexwang/Documents/NLP/Assignment1/argessaycorpus.txt').read()\n",
    "argessay_splited = argessay.split(\" \")\n",
    "argessay_lower = lower(argessay_splited)\n",
    "argessay_lower_wtpun = \" \".join(alphanum(argessay_lower))\n",
    "\n",
    "tokens_arg = word_tokenize(argessay_lower_wtpun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of assertive verb in the research paper corpus is 10\n",
      "the number of assertive verb in the argumentative essay corpus is 25\n",
      "the assertive verbs ratio in the research paper corpus is 0.00025729429321257654\n",
      "the assertive verbs ratio in the argumentative essay corpus is 0.0007032348804500703\n"
     ]
    }
   ],
   "source": [
    "#get a list of verbs in the two corpora\n",
    "from nltk import pos_tag \n",
    "\n",
    "v_words_tag =['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "res_tag = pos_tag(tokens_res)\n",
    "\n",
    "v_res = []\n",
    "for item in res_tag:\n",
    "    if item[1] in v_words_tag:\n",
    "        v_res.append(item[0])        \n",
    "       \n",
    "arg_tag = pos_tag(tokens_arg)\n",
    "\n",
    "v_arg = []\n",
    "for item in arg_tag:\n",
    "    if item[1] in v_words_tag:\n",
    "        v_arg.append(item[0])\n",
    "\n",
    "\n",
    "#calculate assertive verbs in the two corpora\n",
    "\n",
    "assertive_verbs = ['declare', 'contend', 'profess', 'affirm', 'argue', 'aver', 'state', 'uphold', 'claim', 'avow', \n",
    "                   'pronounce', 'protest', 'postulate', 'maintain', 'allege', 'defend', 'attest', 'swear', 'purport',  \n",
    "                   'posit', 'insist', 'proclaim'] \n",
    "\n",
    "\n",
    "#assertive verbs in the two corpora\n",
    "\n",
    "def assertive_count(list):\n",
    "    assertive_in_c = []\n",
    "    for w in list:\n",
    "        if w in assertive_verbs:\n",
    "            assertive_in_c.append(w)\n",
    "    return assertive_in_c \n",
    "\n",
    "print('the number of assertive verb in the research paper corpus is', len(assertive_count(v_res)))\n",
    "print('the number of assertive verb in the argumentative essay corpus is', len(assertive_count(v_arg)))            \n",
    "\n",
    "#assertive verbs ratio in the two corpora \n",
    "print('the assertive verbs ratio in the research paper corpus is', len(assertive_count(v_res))/ len(tokens_res))\n",
    "print('the assertive verbs ratio in the argumentative essay corpus is', len(assertive_count(v_arg))/ len(tokens_arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of adveribal hedges in the research paper corpus is 52\n",
      "the number of adveribal hedges in the argumentative essay corpus is 62\n",
      "the adverbial hedges ratio in the research paper corpus is 0.001337930324705398\n",
      "the adverbial hedges ratio in the argumentative essay corpus is 0.0017440225035161743\n"
     ]
    }
   ],
   "source": [
    "#calculating adverbial hedging \n",
    "\n",
    "adverbial_hedging_list = ['probably','likely','perhaps','possibly','conceivably']\n",
    "\n",
    "def adverbial_hedge_count(list):\n",
    "    adverbial_hedge = []\n",
    "    for h in list:\n",
    "        if h in adverbial_hedging_list:\n",
    "            adverbial_hedge.append(h)\n",
    "    return adverbial_hedge\n",
    "\n",
    "print('the number of adveribal hedges in the research paper corpus is', len(adverbial_hedge_count(tokens_res)))\n",
    "print('the number of adveribal hedges in the argumentative essay corpus is', len(adverbial_hedge_count(tokens_arg)))\n",
    "\n",
    "print('the adverbial hedges ratio in the research paper corpus is', len(adverbial_hedge_count(tokens_res))/ len(tokens_res))    \n",
    "print('the adverbial hedges ratio in the argumentative essay corpus is', len(adverbial_hedge_count(tokens_arg))/ len(tokens_arg))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of passive voice used in the research paper is 495\n",
      "the number of passive voice used in the argumentative essay is 350\n"
     ]
    }
   ],
   "source": [
    "#passive voice \n",
    "\n",
    "import nltk \n",
    "\n",
    "#calculate passive voice in the research paper corpus\n",
    "\n",
    "chunkgram = r\"\"\"Chunk: {<VB.?><VBN><VBN>?}\"\"\"\n",
    "chunkparser = nltk.RegexpParser(chunkgram)\n",
    "result = chunkparser.parse(res_tag)\n",
    "\n",
    "chunk_list_res = []\n",
    "for node in result:\n",
    "     if isinstance(node, Tree):\n",
    "            chunk_list_res.append(node)\n",
    "            \n",
    "be_list = [('was', 'VBD'),('is', 'VBZ'),('are', 'VBP'),('were', 'VBD'),('be', 'VB'),('been', 'VBN'),('being', 'VBG')] \n",
    "res_passive =[]\n",
    "for tree in chunk_list_res:\n",
    "    if tree[0] in be_list:\n",
    "        res_passive.append(tree)\n",
    "\n",
    "print('the number of passive voice used in the research paper is' , len(res_passive))\n",
    "\n",
    "#calculate passive voice in the argumentative essay corpus\n",
    "\n",
    "chunkgram = r\"\"\"Chunk: {<VB.?><VBN><VBN>?}\"\"\"\n",
    "chunkparser = nltk.RegexpParser(chunkgram)\n",
    "result = chunkparser.parse(arg_tag)\n",
    "\n",
    "chunk_list_arg = []\n",
    "for node in result:\n",
    "     if isinstance(node, Tree):\n",
    "            chunk_list_arg.append(node)\n",
    "            \n",
    "be_list = [('was', 'VBD'),('is', 'VBZ'),('are', 'VBP'),('were', 'VBD'),('be', 'VB'),('been', 'VBN'),('being', 'VBG')] \n",
    "arg_passive =[]\n",
    "for tree in chunk_list_arg:\n",
    "    if tree[0] in be_list or len(tree) == 3:\n",
    "        arg_passive.append(tree)\n",
    "\n",
    "print('the number of passive voice used in the argumentative essay is', len(arg_passive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
